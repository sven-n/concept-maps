{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.training.example import Example #Müssen später vermutlich was anderes nehmen\n",
    "from spacy.training.iob_utils import offsets_to_biluo_tags\n",
    "import random\n",
    "import os\n",
    "from got_2NerTrainingData import training_data_got #sehr viele Warnings\n",
    "#from training_set import training_data_got #Adriana. Weniger Warnings, aber man muss komisch durch iterieren \n",
    "\n",
    "# Load the base model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Create and add the NER pipeline component\n",
    "ner = nlp.add_pipe(\"ner\")\n",
    "\n",
    "# Define the labels and add them to the NER component\n",
    "labels = [\"PERSON\"]\n",
    "for label in labels:\n",
    "    ner.add_label(label)\n",
    "\n",
    "#for train_data2 in training_data_got:\n",
    "    # Check entity alignment\n",
    "for text, annotations in training_data_got:\n",
    "    doc = nlp.make_doc(text)\n",
    "    #biluo_tags = offsets_to_biluo_tags(doc, annotations[\"entities\"]) # Mit Svens Daten ging es nicht\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    optimizer = nlp.initialize()\n",
    "\n",
    "# Perform the training\n",
    "for iteration in range(10):  #10 für die kleine Anzahl. Ansonsten könnte man ja so lange iterieren, bis die Fehler eine gewisse Untergrenze erreicht haben\n",
    "    random.shuffle(training_data_got)\n",
    "    losses = {}\n",
    "    for text, annotations in training_data_got:\n",
    "        doc = nlp.make_doc(text)\n",
    "        example = Example.from_dict(doc, annotations)\n",
    "        nlp.update([example], losses=losses, sgd=optimizer)\n",
    "    #print(\"Losses:\", losses)\n",
    "\n",
    "# Save the trained NER model\n",
    "output_dir = os.getcwd()\n",
    "nlp.to_disk(output_dir)\n",
    "\n",
    "# Load the saved model\n",
    "loaded_nlp = spacy.load(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"text.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "# Load the saved model\n",
    "loaded_nlp = spacy.load(output_dir)\n",
    "\n",
    "# Test the loaded NER model\n",
    "doc = loaded_nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_) #Laut Recherche leer, weil Datensatz zu klein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Laden der validierten Daten aus dem JSON-Dateiformat\n",
    "with open('relationships.json', 'r') as file:\n",
    "    validation_data = json.load(file)\n",
    "\n",
    "# Ausgabe des \"data\" Objekts\n",
    "print(validation_data)\n",
    "\n",
    "# Iteriere über die Sätze im Validierungsdatensatz\n",
    "\n",
    "for data in validation_data:\n",
    "    sentence = data['sentence']\n",
    "    entities = data['relationships']\n",
    "    predicted_relationships=[]\n",
    "    for entity in entities:\n",
    "        firstEntity = entity['firstEntity']\n",
    "        #relationshipType = entity['relationshipType'] # bei ner ja eignetlich unnötig\n",
    "        secondEntity = entity['secondEntity']\n",
    "        predicted_relationships.append(firstEntity)\n",
    "        predicted_relationships.append(secondEntity)\n",
    "    predicted_relationships = set(predicted_relationships)\n",
    "    print(predicted_relationships)\n",
    "    # Verarbeite den Satz mit dem geladenen NER-Modell\n",
    "    doc = loaded_nlp(sentence)\n",
    "\n",
    "    # Vergleiche die vorhergesagten Entitäten mit den tatsächlichen Entitäten\n",
    "    predicted_entities = [(ent.text, ent.label_) for ent in doc.ents] # wird leer sein (war oben schon leer) , benötigen einen größeren Datensatz\n",
    "    if predicted_entities == entities: # geht vermutlich bei arrays nicht\n",
    "        print(\"Entitäten: Vorhersage stimmt überein\")\n",
    "    else:\n",
    "        print(\"Entitäten: Vorhersage stimmt nicht überein\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
