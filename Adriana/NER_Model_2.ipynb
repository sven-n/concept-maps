{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install spacy[transformers]\n",
    "\n",
    "import spacy\n",
    " \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    " \n",
    "#print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"What video sharing service did Steve Chen, Chad Hurley, and Jawed Karim create in 2005?\"\n",
    "doc = nlp(text)\n",
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)\n",
    "\n",
    "string = \"Antiretroviral therapy ( ART ) is recommended for all HIV-infected individuals\"\n",
    "doc = nlp(string)\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_set import training_data_got\n",
    "    \n",
    "#print(training_data_got['examples'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_data_got)\n",
    "\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.blank(\"en\") # load a new spacy model\n",
    "doc_bin = DocBin() # create a DocBin object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.util import filter_spans\n",
    "\n",
    "for training_example  in tqdm(training_data_got): \n",
    "    text = training_example['text']\n",
    "    labels = training_example['entities']\n",
    "    doc = nlp.make_doc(text) \n",
    "    ents = []\n",
    "    for start, end, label in labels:\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "        if span is None:\n",
    "            print(\"Skipping entity\")\n",
    "        else:\n",
    "            ents.append(span)\n",
    "    filtered_ents = filter_spans(ents)\n",
    "    doc.ents = filtered_ents \n",
    "    doc_bin.add(doc)\n",
    "\n",
    "doc_bin.to_disk(\"training_data.spacy\") # save the docbin object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m spacy init fill-config base_config.cfg config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m spacy train config.cfg --output ./ --paths.train ./training_data.spacy --paths.dev ./training_data.spacy --gpu-id 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nlp_ner = spacy.load(\"model-best\")\n",
    "\n",
    "doc = nlp_ner(\"Antiretroviral therapy (ART) is recommended for all HIV-infected\\\n",
    "individuals to reduce the risk of disease progression.\\nART also is recommended \\\n",
    "for HIV-infected individuals for the prevention of transmission of HIV.\\nPatients \\\n",
    "starting ART should be willing and able to commit to treatment and understand the\\\n",
    "benefits and risks of therapy and the importance of adherence. Patients may choose\\\n",
    "to postpone therapy, and providers, on a case-by-case basis, may elect to defer\\\n",
    "therapy on the basis of clinical and/or psychosocial factors.\")\n",
    "\n",
    "colors = {\"PATHOGEN\": \"#F67DE3\", \"MEDICINE\": \"#7DF6D9\", \"MEDICALCONDITION\":\"#FFFFFF\"}\n",
    "options = {\"colors\": colors} \n",
    "\n",
    "spacy.displacy.render(doc, style=\"ent\", options= options, jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.blank(\"en\")  # Lade ein neues Spacy-Modell\n",
    "doc_bin = DocBin()  # Erstelle ein DocBin-Objekt\n",
    "\n",
    "from spacy.util import filter_spans\n",
    "\n",
    "for training_example in tqdm(training_data_got):\n",
    "    text = training_example['text']\n",
    "    labels = training_example['entities']\n",
    "    doc = nlp.make_doc(text)\n",
    "    ents = []\n",
    "    for start, end, label in labels:\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "        if span is None:\n",
    "            print(\"Skipping entity\")\n",
    "        else:\n",
    "            ents.append(span)\n",
    "    filtered_ents = filter_spans(ents)\n",
    "    doc.ents = filtered_ents\n",
    "    doc_bin.add(doc)\n",
    "\n",
    "doc_bin.to_disk(\"training_data.spacy\")  # Speichere das DocBin-Objekt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Queen Sansa Stark is the eldest daughter of Lord Eddard Stark and his wife, Lady Catelyn, sister of Robb, Arya, Bran, and Rickon Stark, and \"half-sister\" of Jon Snow; though truthfully is his cousin', {'entities': [(6, 17, 'PERSON'), (157, 165, 'PERSON'), (122, 134, 'PERSON'), (49, 61, 'PERSON')]})\n"
     ]
    }
   ],
   "source": [
    "from training_set import training_data_got\n",
    "print(training_data_got[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'training_data_got'\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "nlp = spacy.blank(\"en\")  # Lade ein neues Spacy-Modell\n",
    "doc_bin = DocBin()  # Erstelle ein DocBin-Objekt\n",
    "\n",
    "text, annotations = training_data_got[0]\n",
    "entities = annotations['entities']\n",
    "\n",
    "doc = nlp.make_doc(text)\n",
    "entities = [(start, end, label) for start, end, label in entities]\n",
    "doc.ents = [doc.char_span(start, end, label=label) for start, end, label in entities if doc.char_span(start, end, label=label)]\n",
    "\n",
    "doc_bin.add(doc)\n",
    "\n",
    "doc_bin.to_disk(\"training_data.spacy\")  # Speichere das DocBin-Objekt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy init fill-config base_config.cfg config.cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory: .\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-05-28 20:41:16,395] [INFO] Set up nlp object from config\n",
      "[2023-05-28 20:41:16,419] [INFO] Pipeline: ['tok2vec', 'ner', 'tagger', 'morphologizer', 'trainable_lemmatizer']\n",
      "[2023-05-28 20:41:16,419] [INFO] Created vocabulary\n",
      "[2023-05-28 20:41:16,419] [INFO] Finished initializing nlp object\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\Adriana\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\__main__.py\", line 4, in <module>\n",
      "    setup_cli()\n",
      "  File \"c:\\Users\\Adriana\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\cli\\_util.py\", line 74, in setup_cli\n",
      "    command(prog_name=COMMAND)\n",
      "  File \"c:\\Users\\Adriana\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\click\\core.py\", line 1130, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Adriana\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\typer\\core.py\", line 778, in main\n",
      "    return _main(\n",
      "           ^^^^^^\n",
      "  File \"c:\\Users\\Adriana\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\typer\\core.py\", line 216, in _main\n",
      "    rv = self.invoke(ctx)\n",
      "         ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Adriana\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\click\\core.py\", line 1657, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Adriana\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\click\\core.py\", line 1404, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Adriana\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\click\\core.py\", line 760, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Adriana\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\typer\\main.py\", line 683, in wrapper\n",
      "    return callback(**use_params)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Adriana\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\cli\\train.py\", line 45, in train_cli\n",
      "    train(config_path, output_path, use_gpu=use_gpu, overrides=overrides)\n",
      "  File \"c:\\Users\\Adriana\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\cli\\train.py\", line 72, in train\n",
      "    nlp = init_nlp(config, use_gpu=use_gpu)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Adriana\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\training\\initialize.py\", line 85, in init_nlp\n",
      "    nlp.initialize(lambda: train_corpus(nlp), sgd=optimizer)\n",
      "  File \"c:\\Users\\Adriana\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\language.py\", line 1308, in initialize\n",
      "    proc.initialize(get_examples, nlp=self, **p_settings)\n",
      "  File \"spacy\\pipeline\\tagger.pyx\", line 305, in spacy.pipeline.tagger.Tagger.initialize\n",
      "  File \"spacy\\pipeline\\pipe.pyx\", line 119, in spacy.pipeline.pipe.Pipe._require_labels\n",
      "ValueError: [E143] Labels for component 'tagger' not initialized. This can be fixed by calling add_label, or by providing a representative batch of examples to the component's `initialize` method.\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy train config.cfg --output ./ --paths.train ./training_data.spacy --paths.dev ./training_data.spacy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory: .\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[38;5;1m✘ Error parsing config overrides\u001b[0m\n",
      "paths -> train\tnot a section value that can be overwritten\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "nlp = spacy.blank(\"en\")  # Lade ein neues Spacy-Modell\n",
    "doc_bin = DocBin()  # Erstelle ein DocBin-Objekt\n",
    "\n",
    "text, annotations = training_data_got[0]\n",
    "entities = annotations['entities']\n",
    "\n",
    "doc = nlp.make_doc(text)\n",
    "entities = [(start, end, label) for start, end, label in entities]\n",
    "doc.ents = [doc.char_span(start, end, label=label) for start, end, label in entities if doc.char_span(start, end, label=label)]\n",
    "\n",
    "doc_bin.add(doc)\n",
    "\n",
    "doc_bin.to_disk(\"training_data.spacy\")  # Speichere das DocBin-Objekt\n",
    "\n",
    "# Initialisiere die Labels für den Tagger\n",
    "ner = nlp.add_pipe(\"ner\")\n",
    "for _, annotations in training_data_got:\n",
    "    for _, _, label in annotations['entities']:\n",
    "        ner.add_label(label)\n",
    "\n",
    "# Konfigurationsdatei erstellen\n",
    "with open(\"config.cfg\", \"w\") as config_file:\n",
    "    config_file.write(\"[nlp]\\n\")\n",
    "    config_file.write(\"lang = en\\n\")\n",
    "    config_file.write(\"pipeline = ner\\n\")\n",
    "    config_file.write(\"[components.ner]\\n\")\n",
    "    config_file.write(\"factory = 'ner'\\n\")\n",
    "    config_file.write(\"[corpora]\\n\")\n",
    "    config_file.write(\"train = './training_data.spacy'\\n\")\n",
    "    config_file.write(\"dev = './training_data.spacy'\\n\")\n",
    "\n",
    "# Training durchführen\n",
    "! python -m spacy train config.cfg --output ./ --paths.train ./training_data.spacy --paths.dev ./training_data.spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[38;5;1m✘ Error parsing config overrides\u001b[0m\n",
      "paths -> train\tnot a section value that can be overwritten\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "import os\n",
    "\n",
    "nlp = spacy.blank(\"en\")  # Lade ein neues Spacy-Modell\n",
    "doc_bin = DocBin()  # Erstelle ein DocBin-Objekt\n",
    "\n",
    "text, annotations = training_data_got[0]\n",
    "entities = annotations['entities']\n",
    "\n",
    "doc = nlp.make_doc(text)\n",
    "entities = [(start, end, label) for start, end, label in entities]\n",
    "doc.ents = [doc.char_span(start, end, label=label) for start, end, label in entities if doc.char_span(start, end, label=label)]\n",
    "\n",
    "doc_bin.add(doc)\n",
    "\n",
    "doc_bin.to_disk(\"training_data.spacy\")  # Speichere das DocBin-Objekt\n",
    "\n",
    "# Initialisiere die Labels für den Tagger\n",
    "ner = nlp.add_pipe(\"ner\")\n",
    "for _, annotations in training_data_got:\n",
    "    for _, _, label in annotations['entities']:\n",
    "        ner.add_label(label)\n",
    "\n",
    "# Konfigurationsdatei erstellen\n",
    "output_dir = \"./output\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "with open(os.path.join(output_dir, \"config.cfg\"), \"w\") as config_file:\n",
    "    config_file.write(\"[nlp]\\n\")\n",
    "    config_file.write(\"lang = en\\n\")\n",
    "    config_file.write(\"pipeline = ner\\n\")\n",
    "    config_file.write(\"[components.ner]\\n\")\n",
    "    config_file.write(\"factory = 'ner'\\n\")\n",
    "    config_file.write(\"[corpora]\\n\")\n",
    "    config_file.write(\"train = './training_data.spacy'\\n\")\n",
    "    config_file.write(\"dev = './training_data.spacy'\\n\")\n",
    "\n",
    "# Training durchführen\n",
    "!python -m spacy train ./output/config.cfg --output ./output --paths.train ./training_data.spacy --paths.dev ./training_data.spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Usage: python -m spacy train [OPTIONS] CONFIG_PATH\n",
      "Try 'python -m spacy train --help' for help.\n",
      "┌─ Error ─────────────────────────────────────────────────────────────────────┐\n",
      "│ Invalid value for 'CONFIG_PATH': Path '--config' does not exist.            │\n",
      "└─────────────────────────────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "import os\n",
    "\n",
    "nlp = spacy.blank(\"en\")  # Lade ein neues Spacy-Modell\n",
    "doc_bin = DocBin()  # Erstelle ein DocBin-Objekt\n",
    "\n",
    "text, annotations = training_data_got[0]\n",
    "entities = annotations['entities']\n",
    "\n",
    "doc = nlp.make_doc(text)\n",
    "entities = [(start, end, label) for start, end, label in entities]\n",
    "doc.ents = [doc.char_span(start, end, label=label) for start, end, label in entities if doc.char_span(start, end, label=label)]\n",
    "\n",
    "doc_bin.add(doc)\n",
    "\n",
    "doc_bin.to_disk(\"training_data.spacy\")  # Speichere das DocBin-Objekt\n",
    "\n",
    "# Initialisiere die Labels für den Tagger\n",
    "ner = nlp.add_pipe(\"ner\")\n",
    "for _, annotations in training_data_got:\n",
    "    for _, _, label in annotations['entities']:\n",
    "        ner.add_label(label)\n",
    "\n",
    "# Konfigurationsdatei verwenden\n",
    "output_dir = \"./output\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "config_template = \"\"\"\n",
    "[paths]\n",
    "train = \"training_data.spacy\"\n",
    "dev = \"training_data.spacy\"\n",
    "output = \"{output_dir}\"\n",
    "\n",
    "[corpora]\n",
    "\n",
    "[nlp]\n",
    "lang = \"en\"\n",
    "pipeline = [\"ner\"]\n",
    "\n",
    "[components]\n",
    "\n",
    "[components.ner]\n",
    "factory = \"ner\"\n",
    "\"\"\"\n",
    "config_path = os.path.join(output_dir, \"config.cfg\")\n",
    "with open(config_path, \"w\") as config_file:\n",
    "    config_file.write(config_template.format(output_dir=output_dir))\n",
    "\n",
    "# Training durchführen\n",
    "!python -m spacy train --config {config_path}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"[E001] No component 'pos' found in pipeline. Available names: ['tok2vec', 'tagger', 'parser', 'senter', 'attribute_ruler', 'lemmatizer', 'ner']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Adriana\\Desktop\\Uni\\Fachpraktikum\\Repo\\concept-maps\\Adriana\\NER_Model_2.ipynb Cell 18\u001b[0m in \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Adriana/Desktop/Uni/Fachpraktikum/Repo/concept-maps/Adriana/NER_Model_2.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39men_core_web_lg\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Adriana/Desktop/Uni/Fachpraktikum/Repo/concept-maps/Adriana/NER_Model_2.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Abrufen der POS-Tagging-Pipeline-Komponente\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Adriana/Desktop/Uni/Fachpraktikum/Repo/concept-maps/Adriana/NER_Model_2.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m pos \u001b[39m=\u001b[39m nlp\u001b[39m.\u001b[39;49mget_pipe(\u001b[39m\"\u001b[39;49m\u001b[39mpos\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adriana/Desktop/Uni/Fachpraktikum/Repo/concept-maps/Adriana/NER_Model_2.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Verwendeter Trainingsdatensatz (Texte und POS-Tags)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adriana/Desktop/Uni/Fachpraktikum/Repo/concept-maps/Adriana/NER_Model_2.ipynb#X24sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m training_data \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\Adriana\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\language.py:620\u001b[0m, in \u001b[0;36mLanguage.get_pipe\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    618\u001b[0m     \u001b[39mif\u001b[39;00m pipe_name \u001b[39m==\u001b[39m name:\n\u001b[0;32m    619\u001b[0m         \u001b[39mreturn\u001b[39;00m component\n\u001b[1;32m--> 620\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE001\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname, opts\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcomponent_names))\n",
      "\u001b[1;31mKeyError\u001b[0m: \"[E001] No component 'pos' found in pipeline. Available names: ['tok2vec', 'tagger', 'parser', 'senter', 'attribute_ruler', 'lemmatizer', 'ner']\""
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "import random\n",
    "\n",
    "# Laden des Basismodells\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Abrufen der POS-Tagging-Pipeline-Komponente\n",
    "pos = nlp.get_pipe(\"pos\")\n",
    "\n",
    "# Verwendeter Trainingsdatensatz (Texte und POS-Tags)\n",
    "\n",
    "training_data = []\n",
    "\n",
    "sentence = \"Queen Sansa Stark is the eldest daughter of Lord Eddard Stark and his wife, Lady Catelyn, sister of Robb, Arya, Bran, and Rickon Stark, and \\\"half-sister\\\" of Jon Snow; though truthfully is his cousin\"\n",
    "# Erzeugen einer leeren SpaCy-Dokumentinstanz\n",
    "doc = nlp.make_doc(sentence)\n",
    "relationships = [\n",
    "    {\n",
    "        \"firstEntity\": \"Sansa Stark\",\n",
    "        \"relationshipType\": \"Father\",\n",
    "        \"secondEntity\": \"Eddard Stark\"\n",
    "    },\n",
    "    {\n",
    "        \"firstEntity\": \"Sansa Stark\",\n",
    "        \"relationshipType\": \"Siblings\",\n",
    "        \"secondEntity\": \"Jon Snow\"\n",
    "    },\n",
    "    {\n",
    "        \"firstEntity\": \"Sansa Stark\",\n",
    "        \"relationshipType\": \"Siblings\",\n",
    "        \"secondEntity\": \"Rickon Stark\"\n",
    "    },\n",
    "    {\n",
    "        \"firstEntity\": \"Jon Snow\",\n",
    "        \"relationshipType\": \"Father\",\n",
    "        \"secondEntity\": \"Eddard Stark\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for relationship in relationships:\n",
    "    first_entity = relationship[\"firstEntity\"]\n",
    "    second_entity = relationship[\"secondEntity\"]\n",
    "    relationship_type = relationship[\"relationshipType\"]\n",
    "    \n",
    "    # Token-IDs der beteiligten Entitäten finden\n",
    "    first_entity_ids = [i for i, token in enumerate(doc) if token.text == first_entity]\n",
    "    second_entity_ids = [i for i, token in enumerate(doc) if token.text == second_entity]\n",
    "    \n",
    "    # Beziehungen für jedes Entitätenpaar hinzufügen\n",
    "    for first_id in first_entity_ids:\n",
    "        for second_id in second_entity_ids:\n",
    "            training_data.append((sentence, {\"entities\": [(first_id, first_id + len(first_entity), \"PERSON\"), \n",
    "                                                         (second_id, second_id + len(second_entity), \"PERSON\")],\n",
    "                                             \"relations\": [(first_id, second_id, relationship_type)]}))\n",
    "                                             \n",
    "# Trainingsdaten ausgeben\n",
    "print(training_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Adriana\\Desktop\\Uni\\Fachpraktikum\\Repo\\concept-maps\\Adriana\\NER_Model_2.ipynb Cell 19\u001b[0m in \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adriana/Desktop/Uni/Fachpraktikum/Repo/concept-maps/Adriana/NER_Model_2.ipynb#X26sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m first_span \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39mvocab\u001b[39m.\u001b[39mstrings[first_entity]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adriana/Desktop/Uni/Fachpraktikum/Repo/concept-maps/Adriana/NER_Model_2.ipynb#X26sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m second_span \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39mvocab\u001b[39m.\u001b[39mstrings[second_entity]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Adriana/Desktop/Uni/Fachpraktikum/Repo/concept-maps/Adriana/NER_Model_2.ipynb#X26sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m first_entity_ids \u001b[39m=\u001b[39m [i \u001b[39mfor\u001b[39;00m i, token \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(doc) \u001b[39mif\u001b[39;00m token\u001b[39m.\u001b[39mtext \u001b[39min\u001b[39;00m first_span]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adriana/Desktop/Uni/Fachpraktikum/Repo/concept-maps/Adriana/NER_Model_2.ipynb#X26sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m second_entity_ids \u001b[39m=\u001b[39m [i \u001b[39mfor\u001b[39;00m i, token \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(doc) \u001b[39mif\u001b[39;00m token\u001b[39m.\u001b[39mtext \u001b[39min\u001b[39;00m second_span]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adriana/Desktop/Uni/Fachpraktikum/Repo/concept-maps/Adriana/NER_Model_2.ipynb#X26sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m# Beziehungen für jedes Entitätenpaar hinzufügen\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\Adriana\\Desktop\\Uni\\Fachpraktikum\\Repo\\concept-maps\\Adriana\\NER_Model_2.ipynb Cell 19\u001b[0m in \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adriana/Desktop/Uni/Fachpraktikum/Repo/concept-maps/Adriana/NER_Model_2.ipynb#X26sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m first_span \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39mvocab\u001b[39m.\u001b[39mstrings[first_entity]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adriana/Desktop/Uni/Fachpraktikum/Repo/concept-maps/Adriana/NER_Model_2.ipynb#X26sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m second_span \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39mvocab\u001b[39m.\u001b[39mstrings[second_entity]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Adriana/Desktop/Uni/Fachpraktikum/Repo/concept-maps/Adriana/NER_Model_2.ipynb#X26sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m first_entity_ids \u001b[39m=\u001b[39m [i \u001b[39mfor\u001b[39;00m i, token \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(doc) \u001b[39mif\u001b[39;00m token\u001b[39m.\u001b[39mtext \u001b[39min\u001b[39;00m first_span]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adriana/Desktop/Uni/Fachpraktikum/Repo/concept-maps/Adriana/NER_Model_2.ipynb#X26sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m second_entity_ids \u001b[39m=\u001b[39m [i \u001b[39mfor\u001b[39;00m i, token \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(doc) \u001b[39mif\u001b[39;00m token\u001b[39m.\u001b[39mtext \u001b[39min\u001b[39;00m second_span]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adriana/Desktop/Uni/Fachpraktikum/Repo/concept-maps/Adriana/NER_Model_2.ipynb#X26sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m# Beziehungen für jedes Entitätenpaar hinzufügen\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_frame.py:1183\u001b[0m, in \u001b[0;36mPyDBFrame.trace_dispatch\u001b[1;34m(self, frame, event, arg)\u001b[0m\n\u001b[0;32m   1181\u001b[0m \u001b[39mif\u001b[39;00m is_line:\n\u001b[0;32m   1182\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_suspend(thread, step_cmd, original_step_cmd\u001b[39m=\u001b[39minfo\u001b[39m.\u001b[39mpydev_original_step_cmd)\n\u001b[1;32m-> 1183\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_wait_suspend(thread, frame, event, arg)\n\u001b[0;32m   1184\u001b[0m \u001b[39melif\u001b[39;00m is_return:  \u001b[39m# return event\u001b[39;00m\n\u001b[0;32m   1185\u001b[0m     back \u001b[39m=\u001b[39m frame\u001b[39m.\u001b[39mf_back\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_frame.py:164\u001b[0m, in \u001b[0;36mPyDBFrame.do_wait_suspend\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdo_wait_suspend\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 164\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_args[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mdo_wait_suspend(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2062\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2059\u001b[0m             from_this_thread\u001b[39m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2061\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads_suspended_single_notification\u001b[39m.\u001b[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001b[1;32m-> 2062\u001b[0m         keep_suspended \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[0;32m   2064\u001b[0m frames_list \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   2066\u001b[0m \u001b[39mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2067\u001b[0m     \u001b[39m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2098\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2095\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_input_hook()\n\u001b[0;32m   2097\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_internal_commands()\n\u001b[1;32m-> 2098\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.01\u001b[39m)\n\u001b[0;32m   2100\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[39mstr\u001b[39m(\u001b[39mid\u001b[39m(frame)))\n\u001b[0;32m   2102\u001b[0m \u001b[39m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "import random\n",
    "\n",
    "# Laden des Basismodells\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Verwendeter Trainingsdatensatz (Texte und POS-Tags)\n",
    "training_data = []\n",
    "\n",
    "sentence = \"Queen Sansa Stark is the eldest daughter of Lord Eddard Stark and his wife, Lady Catelyn, sister of Robb, Arya, Bran, and Rickon Stark, and \\\"half-sister\\\" of Jon Snow; though truthfully is his cousin\"\n",
    "\n",
    "# Verarbeiten des Satzes mit SpaCy, um POS-Tags zu erhalten\n",
    "doc = nlp(sentence)\n",
    "pos_tags = [token.pos_ for token in doc]\n",
    "\n",
    "# Erzeugen der Trainingsdaten mit POS-Tags\n",
    "relationships = [\n",
    "    {\n",
    "        \"firstEntity\": \"Sansa Stark\",\n",
    "        \"relationshipType\": \"Father\",\n",
    "        \"secondEntity\": \"Eddard Stark\"\n",
    "    },\n",
    "    {\n",
    "        \"firstEntity\": \"Sansa Stark\",\n",
    "        \"relationshipType\": \"Siblings\",\n",
    "        \"secondEntity\": \"Jon Snow\"\n",
    "    },\n",
    "    {\n",
    "        \"firstEntity\": \"Sansa Stark\",\n",
    "        \"relationshipType\": \"Siblings\",\n",
    "        \"secondEntity\": \"Rickon Stark\"\n",
    "    },\n",
    "    {\n",
    "        \"firstEntity\": \"Jon Snow\",\n",
    "        \"relationshipType\": \"Father\",\n",
    "        \"secondEntity\": \"Eddard Stark\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for relationship in relationships:\n",
    "    first_entity = relationship[\"firstEntity\"]\n",
    "    second_entity = relationship[\"secondEntity\"]\n",
    "    relationship_type = relationship[\"relationshipType\"]\n",
    "    \n",
    "    # Token-IDs der beteiligten Entitäten finden\n",
    "    first_span = doc.vocab.strings[first_entity]\n",
    "    second_span = doc.vocab.strings[second_entity]\n",
    "    first_entity_ids = [i for i, token in enumerate(doc) if token.text in first_span]\n",
    "    second_entity_ids = [i for i, token in enumerate(doc) if token.text in second_span]\n",
    "    \n",
    "    # Beziehungen für jedes Entitätenpaar hinzufügen\n",
    "    for first_id in first_entity_ids:\n",
    "        for second_id in second_entity_ids:\n",
    "            entities = [\n",
    "                (first_id, first_id + len(first_span), \"PERSON\"),\n",
    "                (second_id, second_id + len(second_span), \"PERSON\")\n",
    "            ]\n",
    "            \n",
    "            # POS-Tags für jedes Token hinzufügen\n",
    "            tokens_with_pos = [(token.text, pos_tags[token.i]) for token in doc]\n",
    "            \n",
    "            # Trainingsbeispiel erstellen und zu den Trainingsdaten hinzufügen\n",
    "            training_example = Example.from_dict(nlp.make_doc(sentence), {\"entities\": entities, \"pos\": tokens_with_pos, \"relations\": [(first_id, second_id, relationship_type)]})\n",
    "            training_data.append(training_example)\n",
    "            \n",
    "# Trainingsdaten ausgeben\n",
    "print(training_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x0000022183855B50>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x0000022183854530>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x00000221832726C0>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x0000022180170DD0>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x00000222E139F650>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x0000022183272420>)]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.12.0-cp311-cp311-win_amd64.whl (1.9 kB)\n",
      "Collecting tensorflow-intel==2.12.0 (from tensorflow)\n",
      "  Downloading tensorflow_intel-2.12.0-cp311-cp311-win_amd64.whl (272.9 MB)\n",
      "     -------------------------------------- 272.9/272.9 MB 6.2 MB/s eta 0:00:00\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.12.0->tensorflow)\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "     -------------------------------------- 126.5/126.5 kB 7.3 MB/s eta 0:00:00\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.12.0->tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=2.0 (from tensorflow-intel==2.12.0->tensorflow)\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow-intel==2.12.0->tensorflow)\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.12.0->tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     ---------------------------------------- 57.5/57.5 kB 3.1 MB/s eta 0:00:00\n",
      "Collecting h5py>=2.9.0 (from tensorflow-intel==2.12.0->tensorflow)\n",
      "  Downloading h5py-3.8.0-cp311-cp311-win_amd64.whl (2.6 MB)\n",
      "     ---------------------------------------- 2.6/2.6 MB 10.4 MB/s eta 0:00:00\n",
      "Collecting jax>=0.3.15 (from tensorflow-intel==2.12.0->tensorflow)\n",
      "  Downloading jax-0.4.10.tar.gz (1.3 MB)\n",
      "     ---------------------------------------- 1.3/1.3 MB 13.6 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.12.0->tensorflow)\n",
      "  Downloading libclang-16.0.0-py2.py3-none-win_amd64.whl (24.4 MB)\n",
      "     ---------------------------------------- 24.4/24.4 MB 9.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in c:\\users\\adriana\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.23.4)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.12.0->tensorflow)\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     ---------------------------------------- 65.5/65.5 kB ? eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\adriana\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\adriana\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\adriana\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (67.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\adriana\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.12.0->tensorflow)\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\adriana\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.4.0)\n",
      "Collecting wrapt<1.15,>=1.11.0 (from tensorflow-intel==2.12.0->tensorflow)\n",
      "  Downloading wrapt-1.14.1.tar.gz (50 kB)\n",
      "     ---------------------------------------- 50.9/50.9 kB ? eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.12.0->tensorflow)\n",
      "  Downloading grpcio-1.54.2-cp311-cp311-win_amd64.whl (4.1 MB)\n",
      "     ---------------------------------------- 4.1/4.1 MB 10.5 MB/s eta 0:00:00\n",
      "Collecting tensorboard<2.13,>=2.12 (from tensorflow-intel==2.12.0->tensorflow)\n",
      "  Downloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
      "     ---------------------------------------- 5.6/5.6 MB 10.6 MB/s eta 0:00:00\n",
      "Collecting tensorflow-estimator<2.13,>=2.12.0 (from tensorflow-intel==2.12.0->tensorflow)\n",
      "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
      "     -------------------------------------- 440.7/440.7 kB 9.2 MB/s eta 0:00:00\n",
      "Collecting keras<2.13,>=2.12.0 (from tensorflow-intel==2.12.0->tensorflow)\n",
      "  Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 10.0 MB/s eta 0:00:00\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-intel==2.12.0->tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 11.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\adriana\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow) (0.40.0)\n",
      "Collecting ml-dtypes>=0.1.0 (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow)\n",
      "  Downloading ml_dtypes-0.1.0-cp311-cp311-win_amd64.whl (120 kB)\n",
      "     -------------------------------------- 120.7/120.7 kB 7.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy>=1.7 in c:\\users\\adriana\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (1.9.3)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow)\n",
      "  Downloading google_auth-2.19.0-py2.py3-none-any.whl (181 kB)\n",
      "     -------------------------------------- 181.3/181.3 kB 5.5 MB/s eta 0:00:00\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow)\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow)\n",
      "  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "     ---------------------------------------- 93.9/93.9 kB 5.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\adriana\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.28.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.0-py3-none-any.whl (2.4 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow)\n",
      "  Downloading Werkzeug-2.3.4-py3-none-any.whl (242 kB)\n",
      "     -------------------------------------- 242.5/242.5 kB 7.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\adriana\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from packaging->tensorflow-intel==2.12.0->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\adriana\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (5.2.0)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow)\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "     -------------------------------------- 181.3/181.3 kB 2.2 MB/s eta 0:00:00\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow)\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\users\\adriana\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.26.12)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow)\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\adriana\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\adriana\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\adriana\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2022.9.24)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\adriana\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.1.1)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow)\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "     ---------------------------------------- 83.9/83.9 kB 4.6 MB/s eta 0:00:00\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     -------------------------------------- 151.7/151.7 kB 8.8 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: jax, wrapt\n",
      "  Building wheel for jax (pyproject.toml): started\n",
      "  Building wheel for jax (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for jax: filename=jax-0.4.10-py3-none-any.whl size=1480617 sha256=163ed78acdab4257ca7dcf6d41dd2d02f69835bef5f2da9d057335f9d866878a\n",
      "  Stored in directory: c:\\users\\adriana\\appdata\\local\\pip\\cache\\wheels\\af\\ad\\a6\\2732752b23e24d5de10c9aabe0d73e31220fc2492af1fa19bd\n",
      "  Building wheel for wrapt (setup.py): started\n",
      "  Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "  Created wheel for wrapt: filename=wrapt-1.14.1-cp311-cp311-win_amd64.whl size=35501 sha256=bc7486a6da705d36276271d8d80895761e9834dec19414480939a1aa25bc0944\n",
      "  Stored in directory: c:\\users\\adriana\\appdata\\local\\pip\\cache\\wheels\\eb\\b6\\fa\\5ab6f4107cad63fa04c54ad78d75bb7035119bdd4f751df5ae\n",
      "Successfully built jax wrapt\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, opt-einsum, oauthlib, ml-dtypes, markdown, keras, h5py, grpcio, google-pasta, gast, astunparse, absl-py, rsa, requests-oauthlib, pyasn1-modules, jax, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.19.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.54.2 h5py-3.8.0 jax-0.4.10 keras-2.12.0 libclang-16.0.0 markdown-3.4.3 ml-dtypes-0.1.0 oauthlib-3.2.2 opt-einsum-3.3.0 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.12.3 tensorboard-data-server-0.7.0 tensorflow-2.12.0 tensorflow-estimator-2.12.0 tensorflow-intel-2.12.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.3.0 werkzeug-2.3.4 wrapt-1.14.1\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense\n",
    "\n",
    "# Beispiel-Daten\n",
    "sentences = [\n",
    "    \"Sansa Stark is the eldest daughter and second child of Lady Catelyn and Lord Eddard Stark, the Warden of the North\",\n",
    "    \"Jon Snow is the son of Lady Lyanna Stark and Prince Rhaegar Targaryen\",\n",
    "    # Weitere Sätze hinzufügen...\n",
    "]\n",
    "\n",
    "relationships = [\n",
    "    \"Eddard Stark is the father of Sansa Stark\",\n",
    "    \"Lyanna Stark is the mother of Jon Snow\",\n",
    "    # Weitere Beziehungssätze hinzufügen...\n",
    "]\n",
    "\n",
    "# Erstellen des Vokabulars\n",
    "all_words = ' '.join(sentences).split()\n",
    "unique_words = list(set(all_words))\n",
    "word_to_index = {word: index for index, word in enumerate(unique_words)}\n",
    "vocab_size = len(unique_words)\n",
    "\n",
    "# Modifizieren der Daten für BiLSTM\n",
    "max_sequence_length = max(len(sentence.split()) for sentence in sentences)\n",
    "input_data = np.zeros((len(sentences), max_sequence_length), dtype='int32')\n",
    "target_data = np.zeros((len(relationships),), dtype='int32')\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    word_indices = [word_to_index[word] for word in sentence.split()]\n",
    "    input_data[i, :len(word_indices)] = word_indices\n",
    "\n",
    "for i, relationship in enumerate(relationships):\n",
    "    target_data[i] = int(\"father\" in relationship)  # 1, wenn \"father\" in der Beziehung vorhanden, sonst 0\n",
    "\n",
    "# Definieren der Modellarchitektur\n",
    "embedding_dim = 100\n",
    "hidden_units = 64\n",
    "\n",
    "input_layer = Input(shape=(max_sequence_length,))\n",
    "embedding_layer = Embedding(vocab_size, embedding_dim)(input_layer)\n",
    "bilstm_layer = Bidirectional(LSTM(hidden_units, return_sequences=False))(embedding_layer)\n",
    "output_layer = Dense(1, activation='sigmoid')(bilstm_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training des Modells\n",
    "model.fit(input_data, target_data, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Verwendung des trainierten Modells\n",
    "test_sentence = \"Eddard Stark is the father of Sansa Stark\"\n",
    "test_word_indices = [word_to_index[word] for word in test_sentence.split()]\n",
    "test_input = np.zeros((1, max_sequence_length), dtype='int32')\n",
    "test_input[0, :len(test_word_indices)] = test_word_indices\n",
    "prediction = model.predict(test_input)\n",
    "\n",
    "print(\"Prediction:\", prediction[0])  # Wahrscheinlichkeit, dass die Beziehung \"father\" ist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "#import crosslingual_coreference\n",
    "\n",
    "! pip install crosslingual-coreference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
